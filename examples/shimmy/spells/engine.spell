# ════════════════════════════════════════════════════════════
# engine.spell - Inference Engine Abstraction Layer
# ════════════════════════════════════════════════════════════
# Shimmy: The 4.8MB OpenAI API Server
# ════════════════════════════════════════════════════════════

~ rust_semantics
~ async_trait_pattern
~ trait_object_dispatch
~ tokio_runtime

# ─────────────────────────────────────────────────────────────
# INCANTATION: Generation Options
# ─────────────────────────────────────────────────────────────
# Parameters for text generation, shared across all backends.

@GenOptions
  ^ Default

  #max_tokens -> usize                 # default = 256
  #temperature -> f32                  # default = 0.7, range [0.0, 2.0]
  #top_p -> f32                        # nucleus sampling, default = 0.9
  #top_k -> i32                        # default = 40
  #repeat_penalty -> f32               # default = 1.1
  #seed -> ?u64                        # optional deterministic seed
  #stream -> bool                      # default = false
  #stop_tokens -> Vec<String>          # generation terminators

  :default() -> Self
    # max_tokens: 256
    # temperature: 0.7
    # top_p: 0.9
    # top_k: 40
    # repeat_penalty: 1.1
    # seed: None
    # stream: false
    # stop_tokens: vec![]

# ─────────────────────────────────────────────────────────────
# INCANTATION: Model Backend Enum
# ─────────────────────────────────────────────────────────────
# Represents the underlying inference backend type.

@ModelBackend
  :LlamaGGUF                           # llama.cpp with GGUF weights
  :HuggingFace                         # transformers via Python bridge
  :Candle                              # pure Rust via candle crate
  :MLX                                 # Apple Silicon Metal
  :SafeTensors                         # pure Rust safetensors

# ─────────────────────────────────────────────────────────────
# INCANTATION: Inference Engine Trait
# ─────────────────────────────────────────────────────────────
# The core abstraction for loading models. All backends implement this.

@InferenceEngine
  ^ Send + Sync
  ^ async_trait

  :load(spec: &@ModelSpec) -> Result<Box<dyn @LoadedModel>>
    # loads model from spec
    # returns LoadedModel trait object
    # errors: ModelNotFound, LoadFailed, OutOfMemory

  ! implementors must be Send + Sync
  ! async for non-blocking model loads

# ─────────────────────────────────────────────────────────────
# INCANTATION: Loaded Model Trait
# ─────────────────────────────────────────────────────────────
# A model that has been loaded into memory and is ready to generate.

@LoadedModel
  ^ Send + Sync
  ^ async_trait

  :generate(
    prompt: &str,
    opts: @GenOptions,
    on_token: ?Box<dyn Fn(String) + Send>
  ) -> Result<String>
    # generates text from prompt
    # if on_token is Some, streams each token via callback
    # returns full generated text
    
  ! on_token callback enables streaming
  ! must be thread-safe (Send + Sync)

# ─────────────────────────────────────────────────────────────
# TRAIT OBJECT DISPATCH
# ─────────────────────────────────────────────────────────────
#
# AppState stores:
#   engine: Box<dyn InferenceEngine>
#
# Which dispatches at runtime to:
#   - LlamaEngine
#   - InferenceEngineAdapter (multi-backend router)
#   - HuggingFaceEngine
#   - MLXEngine
#   - SafeTensorsEngine
#
# This allows hot-swapping backends without recompilation.
